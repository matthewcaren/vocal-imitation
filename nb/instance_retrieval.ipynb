{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c041d2-ac1a-4f09-b42b-25f9a82967c5",
   "metadata": {},
   "source": [
    "# Instance retrieval\n",
    "_This notebook calculates the \"reverse\" mapping, retrieving referent sounds based on a vocal imitation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896ebbc-cb73-4558-a0c5-13de29e295ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from RSA_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668f826-727e-4d13-8736-df9666390350",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e96d2-0fe6-4af4-9bc1-5e9a7ae6e2c7",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad849c4-4c16-4280-a416-b83f651db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENT_DIR = '../data/only_animals/'\n",
    "\n",
    "# list of all referents\n",
    "referents = [d for d in os.listdir(REFERENT_DIR) if (os.path.isfile(os.path.join(REFERENT_DIR, d)) and d[-4:]=='.wav')]\n",
    "\n",
    "N_REFERENTS = len(referents)\n",
    "print(\"total referents:\", N_REFERENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5575b0-cbf3-46ab-b5e8-cebde05da620",
   "metadata": {},
   "source": [
    "## Audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c199f-091a-4cee-94f4-21068d4cbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = SimpleAudioFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfebfbf-3fc6-4111-b605-bfcb61db951d",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85478a47-e3ee-4d9c-a844-0809cb1e356c",
   "metadata": {},
   "source": [
    "## Referent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81286589-f557-47c9-839d-b147cecdf19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found this by testing codeboxes below manually, should be computable..\n",
    "FEAT_LEN = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d0e15-1c65-455e-b1a2-1bd3c58042be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_referent_features():\n",
    "    resampler = T.Resample(20_000, AUDIO_SR)\n",
    "    S_features = torch.zeros((len(referents),) + (FEAT_LEN*N_AUDIO_FEATURES,))\n",
    "    NOISE_EPS = 1e-15\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(referents), total=len(referents)):\n",
    "        sample_audio, sr = torchaudio.load(REFERENT_DIR + sample)\n",
    "        sample_audio = sample_audio.repeat((1,5))      # COPY TO BE >= REF - FOR ONESHOTS\n",
    "        resampled = resampler(sample_audio[0,:])[:AUDIO_SR*SAMPLE_LEN//CONTROL_SR]\n",
    "\n",
    "        # add tiny noise since zeros give log NaNs for derivatives\n",
    "        noised = resampled + torch.randn(AUDIO_SR*SAMPLE_LEN//CONTROL_SR)*NOISE_EPS\n",
    "        features = feature_extractor(noised)\n",
    "        feature_vector = torch.reshape(features, (-1,))\n",
    "        S_features[i,:] = feature_vector\n",
    "    \n",
    "    S_features = torch.nn.functional.normalize(S_features, dim=-1)\n",
    "    torch.save(S_features, '../data/only_animals/audio_features.pt')\n",
    "\n",
    "\n",
    "extract_all_referent_features()\n",
    "\n",
    "S_features = torch.load('../data/only_animals/audio_features.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9cbc5-09b9-47f8-91b2-8c5f97242688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,2))\n",
    "plt.imshow(S_features[:, :], interpolation='nearest', aspect='auto', cmap='turbo')\n",
    "plt.title(\"example referent features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fe612-cb86-4502-8e5b-15ea4feafeb7",
   "metadata": {},
   "source": [
    "## utterance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78f1c7-0614-4692-8744-b6a5d48d4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one\n",
    "U_features = torch.zeros((N_GRID_SEQ_TYPE,)*N_VOCAL_TRACT_CONTROLS + (FEAT_LEN*N_AUDIO_FEATURES,))\n",
    "\n",
    "# extract features for a random one\n",
    "controls_tup = (4,7,6,4,8)\n",
    "U_audio, sr = torchaudio.load(VOCAL_DATA_DIR + f'{'-'.join(map(str,controls_tup))}.wav')\n",
    "features = feature_extractor(U_audio[0,:])\n",
    "U_features[*controls_tup] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca30307-43ef-403d-8407-5494ab31a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_utterace_features():\n",
    "    U_features = torch.zeros((N_GRID_SEQ_TYPE,)*N_VOCAL_TRACT_CONTROLS + (FEAT_LEN*N_AUDIO_FEATURES,))\n",
    "    feature_extractor = SimpleAudioFeatures()\n",
    "\n",
    "    # populate utterance feature tensor; assuming referent filenames of type 3-1-5-3-4.wav\n",
    "    for controls_tup in tqdm(itertools.product(range(0, N_SEQ_TYPES), repeat=N_VOCAL_TRACT_CONTROLS), total=N_SEQ_TYPES**N_VOCAL_TRACT_CONTROLS):\n",
    "        U_audio, sr = torchaudio.load(VOCAL_DATA_DIR + f'{'-'.join(map(str,controls_tup))}.wav')\n",
    "        features = feature_extractor(U_audio[0,:])\n",
    "        U_features[*controls_tup] = features\n",
    "    \n",
    "    torch.save(U_features, '../data/vocal_synth/U_features_raw.pt')\n",
    "\n",
    "# extract all (TAKES A LONG TIME)\n",
    "# extract_all_utterace_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52251b-eefe-4f09-beb9-999c2f106f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch & normalize\n",
    "U_features = torch.load('../data/vocal_synth/U_features_raw.pt', weights_only=True)\n",
    "U_features = torch.nn.functional.normalize(U_features, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c097b21-7458-48c0-ab1c-8ca7472c8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.imshow(U_features[8,4,6,:9,7], interpolation='nearest', aspect='auto', cmap='turbo')\n",
    "plt.colorbar()\n",
    "plt.title('example utterance features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d438f66-e6ea-42d5-bf36-71df1f123c35",
   "metadata": {},
   "source": [
    "## Utterance Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221e2aa-babf-41b6-9710-17c6f51ad137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from SF_data_gen\n",
    "def lookup_grid_seq(seq_order, length):\n",
    "    # constant\n",
    "    if seq_order <= 3:\n",
    "        return 0.33*seq_order*torch.ones(SAMPLE_LEN)\n",
    "    # sine\n",
    "    elif seq_order <= 6:\n",
    "        return 0.5 + 0.4*torch.sin(torch.linspace(0,SAMPLE_LEN//CONTROL_SR,steps=SAMPLE_LEN)*2*math.pi*(seq_order-3))\n",
    "    # sine biased hi\n",
    "    elif seq_order <= 7:\n",
    "        return 0.75 + 0.25*torch.sin(torch.linspace(0,SAMPLE_LEN//CONTROL_SR,steps=SAMPLE_LEN)*2*math.pi*(seq_order-5))\n",
    "    # sine biased low\n",
    "    elif seq_order <= 8:\n",
    "        return 0.25 + 0.25*torch.sin(torch.linspace(0,SAMPLE_LEN//CONTROL_SR,steps=SAMPLE_LEN)*2*math.pi*(seq_order-6))\n",
    "    # falling saw\n",
    "    elif seq_order <= 9:\n",
    "        return 1 - (torch.linspace(0, SAMPLE_LEN*(seq_order-7), steps=SAMPLE_LEN) % 1)\n",
    "    # random\n",
    "    else:\n",
    "        return torch.rand(SAMPLE_LEN//3 + 3).repeat_interleave(3)[:SAMPLE_LEN]\n",
    "\n",
    "\n",
    "# calculate derivative & squared dist from 0.5 for each sequence\n",
    "seq_costs = []\n",
    "\n",
    "for i in range(N_GRID_SEQ_TYPE):\n",
    "    seq = lookup_grid_seq(i, CONTROL_SR*2)\n",
    "    deriv = abs(seq - seq.roll(1))\n",
    "    dist = (seq - 0.5)**2  * 0.7\n",
    "    seq_costs.append(torch.sum(deriv) + torch.sum(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cddb0-e030-4b94-8fb9-727fb72a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_costs = torch.zeros((N_GRID_SEQ_TYPE,)*N_VOCAL_TRACT_CONTROLS)\n",
    "\n",
    "# populate cost tensor\n",
    "for controls_tup in itertools.product(range(0, N_GRID_SEQ_TYPE), repeat=N_VOCAL_TRACT_CONTROLS):\n",
    "    cost = torch.sum(torch.tensor([seq_costs[seq_type] for seq_type in controls_tup]))\n",
    "    U_costs[*controls_tup] = cost\n",
    "\n",
    "# normalize to [0, 1]\n",
    "U_costs /= torch.max(U_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cd8c1-24a7-407d-b6de-696df38c6cd2",
   "metadata": {},
   "source": [
    "## Ontological Distance (utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72646fbd-06cd-4d9e-9a9d-7c16c8265a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ontology datastructure\n",
    "tree = build_ontology_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d3cf1-9f4e-4d91-8236-2a7346cbaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds = [\n",
    "    ('shout', '/m/07p6fty'),\n",
    "    ('whispering', '/m/02rtxlg'),\n",
    "    ('crowd', '/m/03qtwd'),\n",
    "    ('car passing by', '/t/dd00134'),\n",
    "]\n",
    "\n",
    "# print(\"path to shout:\", find_key(tree, sounds[2][1]), \"\\n\")\n",
    "for i in range(len(sounds)):\n",
    "    for j in range(i, len(sounds)):\n",
    "        dist = get_ontology_dist(tree, sounds[i][0], sounds[j][0])\n",
    "        print(f\"{sounds[i][0]} <> {sounds[j][0]}:\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680a621-9556-4772-975e-a53059f60096",
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_TO_CAT_PATH = '../data/pickles/FSD50k_categories.pickle'\n",
    "\n",
    "def find_all_referent_categories():\n",
    "    referent_to_category = {}\n",
    "\n",
    "    # find categories\n",
    "    with open(REFERENT_METADATA_PATH, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)  # skip header\n",
    "        for row in tqdm(reader, total=10231):\n",
    "            category_paths = [(find_key(tree, cat), cat) for cat in row[1].split(',')]\n",
    "            most_specific_category = max(category_paths)[1]\n",
    "            referent_to_category[row[0]] = most_specific_category\n",
    "\n",
    "    # save to file\n",
    "    with open(REF_TO_CAT_PATH, 'wb') as handle:\n",
    "        pickle.dump(referent_to_category, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# find_all_referent_categories()\n",
    "\n",
    "with open(REF_TO_CAT_PATH, 'rb') as handle:\n",
    "    referent_to_category = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518f749-2b5f-4084-986d-711169aa5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each referent, find ontological distance to other referents - upper triangular matrix\n",
    "\n",
    "def calculate_all_cross_referent_distances():\n",
    "    # DP cache - this is horrifically inefficient otherwise\n",
    "    known_dict = {}\n",
    "    referent_dists = torch.zeros((N_REFERENTS, N_REFERENTS))\n",
    "    for i, j in tqdm(itertools.combinations(range(N_REFERENTS), 2), total = N_REFERENTS**2/2):\n",
    "        categoryA = referent_to_category[referents[i].split('.')[0]]\n",
    "        categoryB = referent_to_category[referents[j].split('.')[0]]\n",
    "        if (categoryA, categoryA) in known_dict:\n",
    "            referent_dists[i][j] = known_dict[(categoryA, categoryA)]\n",
    "        else:\n",
    "            dist = get_ontology_dist(tree, categoryA, categoryB)\n",
    "            referent_dists[i][j] = dist\n",
    "            known_dict[(categoryA, categoryA)] = dist\n",
    "    torch.save(referent_dists, '../data/only_animals/cross_category_distances.pt')\n",
    "\n",
    "\n",
    "# calculate_all_cross_referent_distances()\n",
    "# cross_ref_dists = torch.load('../data/only_animals/cross_category_distances.pt', weights_only=True)\n",
    "\n",
    "# upper triangular to symmetric\n",
    "# cross_ref_dists = torch.where(cross_ref_dists != 0, cross_ref_dists, cross_ref_dists.T)\n",
    "\n",
    "\n",
    "# no ont distance factor\n",
    "cross_ref_dists = torch.ones((N_REFERENTS, N_REFERENTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c6d03-3e5b-4ae0-905d-21b869636217",
   "metadata": {},
   "source": [
    "## RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa9d89-979c-4285-82d9-7989d15aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speed\n",
    "U_features.to(device)\n",
    "S_features.to(device)\n",
    "cross_ref_dists.to(device)\n",
    "U_costs.to(device)\n",
    "\n",
    "# for convenience\n",
    "NTN = torch.nan_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffa1e8-0797-42c7-9661-6c71542b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meaning matrix\n",
    "meaning = torch.mm(torch.reshape(U_features, (N_GRID_SEQ_TYPE**N_VOCAL_TRACT_CONTROLS, FEAT_LEN*N_AUDIO_FEATURES)), S_features.T)\n",
    "meaning = NTN(meaning)\n",
    "\n",
    "assert list(meaning.shape) == [N_GRID_SEQ_TYPE**N_VOCAL_TRACT_CONTROLS, N_REFERENTS], \\\n",
    "       \"incorrect meaning matrix shape dimension; this probably means we need to recompute referent features for this domain\" \n",
    "\n",
    "print(f\"total utterance-referent pairs: {meaning.shape[0]*meaning.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02665c49-af04-4b04-9254-4a289a261cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONT_DIST_PENALTY = 0.8   # higher penalty = sharper falloff w/ distance\n",
    "\n",
    "cross_ref_dists = torch.exp(-ONT_DIST_PENALTY*cross_ref_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe379a5-3373-4274-ab9c-7f30341adef9",
   "metadata": {},
   "source": [
    "_Note:_\\\n",
    "`meaning`: (utterances, referents)\\\n",
    "`cross_ref_dists`: (referents, referents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f31b9-ce4d-416f-a7c9-6c7e8b494a9b",
   "metadata": {},
   "source": [
    "### Literal listener + Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf836fe3-4ec8-4292-92ea-639db8f5aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FULL RSA\n",
    "\n",
    "COST_FACTOR = 0.2\n",
    "\n",
    "def calculate_utility():\n",
    "    literal_listener = NTN(meaning / torch.sum(meaning, 0, keepdim=True))\n",
    "    \n",
    "    utility = torch.zeros_like(literal_listener)\n",
    "    for i in tqdm(range(N_REFERENTS)):\n",
    "        distances_to_referent = cross_ref_dists[:,i].unsqueeze(0)  \n",
    "        distances_to_referent = distances_to_referent.repeat((N_GRID_SEQ_TYPE**N_VOCAL_TRACT_CONTROLS, 1))  # copy for all utterances\n",
    "        referent_dist_likelihood = distances_to_referent*literal_listener   # elem (ref, ut) = ref to referent i * prob that ut selects ref\n",
    "        utility[:,i] = referent_dist_likelihood.sum(dim=1)    # sum total distance\n",
    "\n",
    "    torch.save(utility, '../data/only_animals/utility.pt')\n",
    "\n",
    "calculate_utility()\n",
    "\n",
    "meaning = torch.load('../data/only_animals/utility.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838c5a0-2b41-4b3c-8a43-a387eb2b8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pragmatic_speaker = NTN(meaning / torch.sum(meaning, 1, keepdim=True))\n",
    "# pragmatic_listener = NTN(pragmatic_speaker / torch.sum(pragmatic_speaker, 0, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9486c-bca8-449a-98e6-bc5ead76a1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utilities = torch.reshape(pragmatic_speaker, (N_GRID_SEQ_TYPE,)*N_VOCAL_TRACT_CONTROLS + (len(referents),))\n",
    "\n",
    "for i, sample in enumerate(referents):\n",
    "    probs = utilities[:,:,:,:,:,i]\n",
    "    best = torch.argmax(probs)\n",
    "    best_ID = '-'.join(map(str, [x.item() for x in torch.unravel_index(best, probs.shape)]))\n",
    "    print('\\nReferent:', sample.split('_')[-2], ',  Utterance:', best_ID)\n",
    "\n",
    "    # samples, sr = torchaudio.load(VOCAL_DATA_DIR + best_ID + '.wav')\n",
    "    # audioelem = Audio(samples[0,:], rate=sr)\n",
    "    # display(audioelem)\n",
    "    # samples, sr = torchaudio.load(REFERENT_DIR + sample)\n",
    "    # audioelem = Audio(samples[0,:], rate=sr)\n",
    "    # display(audioelem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c058ee-228a-4837-9764-9dcced716669",
   "metadata": {},
   "source": [
    "## Matching human imitations to synth utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d44e9c-5a74-4382-b2b0-4a95a91b7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "VI_TO_CAT_PATH = '../data/vocal_imitations/categories.csv'\n",
    "\n",
    "def find_all_vi_categories():\n",
    "    vi_to_category = {}\n",
    "\n",
    "    # find categories\n",
    "    with open(VI_TO_CAT_PATH, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)  # skip header\n",
    "        for row in reader:\n",
    "            vi = row[1]\n",
    "            cat = [x for x in row[3:9] if x][-1]\n",
    "            vi_to_category[vi] = cat\n",
    "\n",
    "    return vi_to_category\n",
    "\n",
    "vi_to_category = find_all_vi_categories()\n",
    "\n",
    "\n",
    "# from RSA run\n",
    "ref_to_ut = np.load(\"../data/pickles/ref_ut_mapping.npy\")\n",
    "\n",
    "mapped_utterances = set([x[1] for x in ref_to_ut])\n",
    "ut_to_ref = {row[1]:row[0] for row in ref_to_ut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dcd88f-b4d6-42ae-8a71-4301b3ce6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_tree = build_ontology_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068fb24-913d-49a5-8eb1-de1b99738d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VI_I = 20\n",
    "vi_path = all_vi[VI_I]\n",
    "\n",
    "vi_features[VI_I].unsqueeze(1).shape\n",
    "\n",
    "synth_utterance_sim = U_features.reshape(11**5, 836) @ vi_features[VI_I]\n",
    "synth_utterance_sim = torch.nan_to_num(synth_utterance_sim.reshape(11,11,11,11,11))\n",
    "\n",
    "top_similarities, top_synth_utterances = torch.topk(synth_utterance_sim.flatten(), 110000)\n",
    "top_synth_utterances_locs = [torch.unravel_index(x, synth_utterance_sim.shape) for x in top_synth_utterances]\n",
    "top_synth_utterances_paths = ['-'.join([str(int(ix.detach())) for ix in utterance]) for utterance in top_synth_utterances_locs]\n",
    "\n",
    "top_mapped_utterances = [u for u in top_synth_utterances_paths if u in mapped_utterances]\n",
    "\n",
    "print(\"intended referent:\", vi_path)\n",
    "# print(\"top synth matches:\", top_mapped_utterances)\n",
    "\n",
    "top_referents = [ut_to_ref[u] for u in top_mapped_utterances]\n",
    "print(\"top referents (unconstrained) \", top_referents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
